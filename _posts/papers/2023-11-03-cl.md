---
layout: paper
categories: papers
permalink: papers/cl
id: cl
title: "On the Diminishing Returns of Width for Continual Learning"
venue: Under Review at AISTATS 2024
url: /papers/cl
pdf: /papers/Go_Sparse_and_Wide_to_Forget_Less.pdf
feature-title: Continual Learning
feature-description: "Continual Learning"
feature-order: 3
featured: true
selected: true
type: conference
authors:
    - Etash Guha
    - Vihan Lakshman
---
While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We prove that width is directly related to forgetting in Feed-Forward Networks (FFN), demonstrating the diminishing returns of increasing widths to reduce forgetting. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.

